# ğŸ¤– MCGG-JAKE â€“ IA que aprende a jugar Magic Chess: Go Go

Bienvenido al repositorio oficial de **mcgg-jake**, un proyecto experimental de inteligencia artificial que aprende a jugar **Magic Chess: Go Go** como lo harÃ­a un humano: observando, interpretando y tomando decisiones sin reglas preprogramadas.

Antes de ejecutar cualquier script, instala todas las dependencias con:

```bash
pip install -r requirements.txt
```

En algunas plataformas `pyautogui` puede requerir paquetes del sistema (por ejemplo `scrot` y `xsel` en Linux).

---

## ğŸ§  Â¿CÃ“MO FUNCIONARÃ LA IA DE MCGG-JAKE?

La IA sigue este ciclo general:

```
[Captura del estado del juego] â†’ [Interpreta quÃ© pasa en pantalla] â†’ [Decide la mejor acciÃ³n] â†’ [Toma la acciÃ³n] â†’ [Observa el resultado] â†’ [Aprende de la experiencia]
```

---

### 1. ğŸ“¸ CAPTURA DEL ESTADO DEL JUEGO

**Entrada:** imÃ¡genes del juego (frames o capturas de pantalla).

**Â¿QuÃ© detecta la IA en cada imagen?**

- Tu oro.
- Nivel actual del jugador.
- Cartas ofrecidas.
- Posiciones de hÃ©roes.
- Cofre del destino (cuando aparece).
- Ãtems y sinergias.

**Â¿CÃ³mo se usan estas imÃ¡genes?**

- Se guardan con su timestamp.
- Se emparejan con la acciÃ³n que la IA tomÃ³ (o debiÃ³ tomar).

---

### 2. ğŸ§© INTERPRETACIÃ“N DEL ESTADO (como un humano)

**NO usamos reglas preprogramadas.**

âœ… En su lugar, usamos **visiÃ³n computacional + aprendizaje automÃ¡tico**.

La IA aprende a identificar:

- QuÃ© es una carta.
- CuÃ¡ndo se gana oro.
- CÃ³mo cambia la tienda.
- QuÃ© decisiones llevan a la victoria.

---

### 3. ğŸ•¹ï¸ DECISIÃ“N DE ACCIÃ“N

La IA debe aprender a decidir segÃºn el contexto:

| Momento             | Posibles Acciones                                 |
|---------------------|---------------------------------------------------|
| Tienda abierta      | Comprar carta A, B, C, o saltar                   |
| Fase de batalla     | No hacer nada                                     |
| Fase de tablero     | Colocar hÃ©roe, cambiar posiciÃ³n                   |
| Cofre del destino   | Elegir 1 de 3                                     |
| Entre rondas        | Actualizar tienda, subir de nivel, guardar oro   |

---

### 4. ğŸ“² EJECUCIÃ“N DE LA ACCIÃ“N

Luego de decidir, la IA:

- Simula un toque en la pantalla (via `pyautogui` o `adb`).
- Espera y observa el siguiente estado.

---

### 5. ğŸ§ª OBSERVACIÃ“N Y APRENDIZAJE

Se utiliza **aprendizaje por refuerzo**:

- Cada acciÃ³n genera **recompensa** o **castigo**.

Ejemplos:

- âœ… Ganar una ronda tras una decisiÃ³n â†’ `+1`
- âŒ Perder oro innecesariamente â†’ `-1`

Con el tiempo, la IA **maximiza su recompensa**.

---

### 6. ğŸ” ENTRENAMIENTO CONTINUO

- Comienza con acciones aleatorias (exploraciÃ³n).
- Luego repite decisiones ganadoras (explotaciÃ³n).
- Se entrena con partidas almacenadas (frames + decisiones + resultados).

---

## ğŸ’¡ EJEMPLO DE CICLO COMPLETO

```
ğŸ–¼ï¸ Frame 0341 del juego: la IA ve una tienda con Miya, Eudora y Layla.
ğŸ¤” Decide: â€œCompro Miyaâ€.
ğŸ–±ï¸ Simula clic en Miya.
â±ï¸ Espera y observa: Â¿GanÃ³ esa ronda? Â¿FormÃ³ sinergia?
ğŸ“ˆ Si fue buena decisiÃ³n â†’ la refuerza. Si no â†’ la evita.
```

---

## ğŸ”§ Â¿QUÃ‰ HAREMOS PRIMERO?

1. Grabar y dividir una partida en frames.
2. Etiquetar acciones por frame (opcional, al principio).
3. Crear dataset: imagen â†’ acciÃ³n â†’ resultado.
4. Entrenar modelo con aprendizaje por refuerzo.
5. Ejecutar acciones reales mÃ¡s adelante.

---

## âœ… BENEFICIOS DEL ENFOQUE

- âŒ No depende de OCR ni reglas fijas.
- âœ… Aprende como un humano: viendo, probando y repitiendo.
- ğŸ› ï¸ Se adapta incluso si el juego cambia ligeramente.

---

## ğŸ“ ESTRUCTURA DEL REPO

- `leer_estado_juego.py` â€“ Captura y anÃ¡lisis del estado actual.
- `motor_decisiones.py` â€“ Modelo que decide acciones (en desarrollo).
- `oro_observador.py`, `leer_oro_automatico.py` â€“ Sistema de detecciÃ³n de oro.
- `modelo_*.pth` â€“ Pesos de los modelos entrenados.
- `mcgg_jake_runner.py` â€“ Ciclo automatizado (por integrar).
- `detection.py` â€“ Entrenamiento y uso del detector de objetos.

## ğŸ¯ Entrenar el detector de objetos

1. Coloca tus capturas anotadas en `dataset/images` y las
   anotaciones en `dataset/annotations.json` (formato simple).
   Ahora se admiten clases adicionales para **cada hÃ©roe en tienda**,
   **las unidades del banco**, **el nivel del jugador** y objetos como
   cofres o Ã­tems.
2. Ejecuta `python detection.py` con la funciÃ³n `train_detector` para
   generar `detector.pth` con todas estas clases.
3. Los mÃ³dulos `leer_oro_automatico.py`, `leer_ronda_automatica.py`,
   `detectar_sinergias.py` y `leer_estado_juego.py` usarÃ¡n ese modelo
   para localizar cada elemento sin depender de la resoluciÃ³n.
4. Para habilitar el aprendizaje continuo ejecuta `python autoentrenar_detector.py`.
   Este proceso captura nuevas imÃ¡genes etiquetadas durante las partidas y
   reentrena periÃ³dicamente, reemplazando `detector.pth` sin intervenciÃ³n manual.
5. Si deseas un ciclo totalmente autÃ³nomo (detector + agente DQN), ejecuta
   `python entrenamiento_autonomo.py`. Este script inicia el capturador y el
   bucle de entrenamiento para que el sistema juegue y aprenda sin supervisiÃ³n.
6. Si los clics no coinciden con tu dispositivo, ajusta las constantes
   `SHOP_SLOT_COORDS` y `FIN_PARTIDA_REGION` en `config.py` segÃºn la resoluciÃ³n
   de pantalla.

---

## âš™ï¸ REQUISITOS

 - Python 3.10+
- `opencv-python`
- `pytesseract`
- `pyautogui`
- `Pillow`
- `torch` (en fase de entrenamiento)
- Instala todas estas dependencias con `pip install -r requirements.txt`

## Modo mÃ³vil

- Instala `scrcpy` o abre un emulador de Android.
- Ejecuta `adb start-server` y luego `adb devices` para verificar la conexiÃ³n.
- Si se detecta tu dispositivo, ejecuta `scrcpy` para ver la pantalla en tiempo real.
- Si `adb devices` no muestra tu mÃ³vil, revisa los drivers USB y activa la depuraciÃ³n USB.

---

## ğŸ“œ LICENCIA

Distribuido bajo la licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.

---

**Creado con visiÃ³n, pasiÃ³n y obsesiÃ³n por automatizar Magic Chess.**
